{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce as sf\n",
    "from simple_salesforce import bulk\n",
    "import teradatasql\n",
    "\n",
    "\n",
    "login_sf = sf(username = 'intl_sales_ops_analysts@groupon.com', password = 'BA_intl_grp_3', security_token = 'ryDPBLSTkb54qbEY06GJSiA0')\n",
    "con = teradatasql.connect(host='tdwd.group.on',user='ub_intl_sales_ops_2',password='BA_intl_grp_4')\n",
    "\n",
    "countries = ['ES','FR','IT','DE','NL','BE','PL','AE','AU','GB','IE']  \n",
    "\n",
    "for country in countries:\n",
    "\n",
    "\n",
    "    ### This loads in the Teradata dataset\n",
    "    td_data = pd.read_sql(f\"SELECT account_id,pds_boost_flag,required_service,load_date FROM sandbox.pds_boost_account_flags WHERE load_date = (SELECT Max(load_date) FROM sandbox.pds_boost_account_flags) AND CASE when feature_country = 'UK' then 'GB' else feature_country end = '{country}'\",con)\n",
    "    td_data.columns = ['td_id','td_PDS_Boost__c','td_PDS_Boost_Summary__c','td_PDS_Boost_Date__c']\n",
    "    \n",
    "    td_data['td_PDS_Boost__c'] = td_data['td_PDS_Boost__c'].replace([1,0],['True','False'])\n",
    "    td_data['td_PDS_Boost_Date__c'] = td_data['td_PDS_Boost_Date__c'].astype(str)\n",
    "    td_data['td_PDS_Boost_Summary__c'] = td_data['td_PDS_Boost_Summary__c'].str[:255]\n",
    "    \n",
    "    ### This loads in the Salesforce dataset\n",
    "    soql_output = login_sf.bulk.Account.query(f\"SELECT Account_ID_18__c,PDS_Boost_Summary__c,PDS_Boost_Date__c,PDS_Boost__c FROM Account WHERE Feature_Country__c = '{country}' AND PDS_Boost_Summary__c != Null limit 200000\")\n",
    "\n",
    "\n",
    "    dic = {}\n",
    "    dic['sf_id'] = [ x['Account_ID_18__c'] for x in soql_output ]\n",
    "    dic['sf_PDS_Boost_Summary__c'] = [ x['PDS_Boost_Summary__c'] for x in soql_output ]\n",
    "    dic['sf_PDS_Boost_Date__c'] = [ x['PDS_Boost_Date__c'] for x in soql_output ]\n",
    "    dic['sf_PDS_Boost__c'] = [ x['PDS_Boost__c'] for x in soql_output ]\n",
    "        \n",
    "    sf_data = pd.DataFrame(dic, columns=dic.keys())\n",
    "    sf_data['sf_PDS_Boost_Date__c'] = sf_data['sf_PDS_Boost_Date__c'].astype(str)\n",
    "    sf_data['sf_PDS_Boost_Summary__c'] = sf_data['sf_PDS_Boost_Summary__c'].str[:255]\n",
    "    \n",
    "\n",
    "    ### Built two different datasets, one for changes to be uploaded (df) and one for values to be deleted (df_del)\n",
    "    df = td_data.merge(sf_data, left_on='td_id', right_on='sf_id', how='left')\n",
    "    df_del = sf_data.merge(td_data, left_on='sf_id', right_on='td_id', how='left')\n",
    "\n",
    "    ### Filter the datasets built so we limit the changes to be made\n",
    "    df2 = df[((df['sf_PDS_Boost_Summary__c'] == df['td_PDS_Boost_Summary__c']) & (df['sf_PDS_Boost_Date__c'].astype(str) != df['td_PDS_Boost_Date__c'].astype(str)))] # data where only a date change is needed\n",
    "    df3 = df[(df['sf_PDS_Boost_Summary__c'] != df['td_PDS_Boost_Summary__c'])] # data where a full change is needed\n",
    "    df_del = df_del[(df_del['td_id'] == None)]\n",
    "\n",
    "\n",
    "    ### Data prep the deletion data\n",
    "    # this drops unnecesary columns\n",
    "    df_del.drop('td_id', axis=1, inplace=True)\n",
    "    df_del.drop('td_PDS_Boost__c', axis=1, inplace=True)\n",
    "    df_del.drop('td_PDS_Boost_Date__c', axis=1, inplace=True)\n",
    "    df_del.drop('td_PDS_Boost_Summary__c', axis=1, inplace=True)\n",
    "\n",
    "    # this adjusts the values in order to be ready for upload\n",
    "    df_del.columns = ['id','PDS_Boost_Summary__c','PDS_Boost_Date__c','PDS_Boost__c']\n",
    "    df_del['PDS_Boost__c'] = 'false'\n",
    "    df_del['PDS_Boost_Summary__c'] = None\n",
    "    df_del['PDS_Boost_Date__c'] = None\n",
    "\n",
    "    if not df_del.empty:\n",
    "        # prep data for upload and upload to SF\n",
    "        ready_for_upload = []\n",
    "\n",
    "        for row in df_del.itertuples():\n",
    "            d = row._asdict()\n",
    "            del d['Index']\n",
    "            ready_for_upload.append(d)\n",
    "            \n",
    "        login_sf.bulk.Account.update(ready_for_upload,batch_size=10000,use_serial=True)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ### Data prep the data where only a date change is needed\n",
    "    # this drops unnecesary columns\n",
    "    df2.drop('sf_id', axis=1, inplace=True)\n",
    "    df2.drop('td_PDS_Boost__c', axis=1, inplace=True)\n",
    "    df2.drop('sf_PDS_Boost__c', axis=1, inplace=True)\n",
    "    df2.drop('sf_PDS_Boost_Summary__c', axis=1, inplace=True)\n",
    "    df2.drop('td_PDS_Boost_Summary__c', axis=1, inplace=True)\n",
    "    df2.drop('sf_PDS_Boost_Date__c', axis=1, inplace=True)\n",
    "\n",
    "    # columns are renamed to fit Salesforce names\n",
    "    df2.columns = ['Account_ID_18__c','PDS_Boost_Date__c']\n",
    "\n",
    "    if not df2.empty:\n",
    "        # prep data for upload and upload to SF\n",
    "        ready_for_upload = []\n",
    "\n",
    "        for row in df2.itertuples():\n",
    "            d = row._asdict()\n",
    "            del d['Index']\n",
    "            ready_for_upload.append(d)\n",
    "\n",
    "        login_sf.bulk.Account.update(ready_for_upload,batch_size=10000,use_serial=True)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ### Data prep the data where a full change is needed\n",
    "    # this drops unnecesary columns    \n",
    "    df3.drop('sf_id', axis=1, inplace=True)\n",
    "    df3.drop('sf_PDS_Boost__c', axis=1, inplace=True)\n",
    "    df3.drop('sf_PDS_Boost_Date__c', axis=1, inplace=True)\n",
    "    df3.drop('sf_PDS_Boost_Summary__c', axis=1, inplace=True)\n",
    "\n",
    "    df3.columns = ['id','PDS_Boost__c','PDS_Boost_Summary__c','PDS_Boost_Date__c']\n",
    "\n",
    "    df3['PDS_Boost_Date__c'] = df3['PDS_Boost_Date__c'].astype(str)\n",
    "    df3['PDS_Boost__c'] = df3['PDS_Boost__c'].astype(bool)\n",
    "\n",
    "    if not df3.empty:\n",
    "        # prep data for upload and upload to SF\n",
    "        ready_for_upload = []\n",
    "\n",
    "        for row in df3.itertuples():\n",
    "            d = row._asdict()\n",
    "            del d['Index']\n",
    "            ready_for_upload.append(d)\n",
    "\n",
    "        login_sf.bulk.Account.update(ready_for_upload,batch_size=10000,use_serial=True)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    print(str(len(df3))+' full changes made in SF for '+country)\n",
    "    print(str(len(df2))+' date changes made in SF for '+country)\n",
    "    print(str(len(df_del))+' values deleted from SF for '+country)\n",
    "    print('Process completed for '+country)\n",
    "\n",
    "print('Process completed')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414c148522c3452b8bca3bc99b333facd4b5aaa0d0e2364d6105bb7336ba2f62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
