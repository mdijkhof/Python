{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22703 accounts will be processed in 12 batches of 2000\n"
     ]
    }
   ],
   "source": [
    "# last update on 2022-11-30\n",
    "\n",
    "import pandas as pd\n",
    "from simple_salesforce import bulk\n",
    "from simple_salesforce import Salesforce as sf\n",
    "import teradatasql\n",
    "import math\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "\n",
    "### This is the BUMBLEBEE output in a dataframe\n",
    "# connect and query Teradata\n",
    "con = teradatasql.connect(host='tdwd.group.on',user='ub_intl_sales_ops',password='BA_intl_grp_4a')\n",
    "\n",
    "bumblebee = pd.read_sql('SELECT DISTINCT account_id,current_owner_id,employee_sf_id FROM sandbox.bumblebee_output where employee_sf_id IS NOT NULL',con)\n",
    "bumblebee['account_id'] = bumblebee['account_id'].str.strip()\n",
    "\n",
    "### This is the current account ownership in SF\n",
    "account_list = bumblebee['account_id'].tolist()\n",
    "\n",
    "batch_size = 2000\n",
    "num_of_acc = len(account_list)\n",
    "num_of_batches = math.ceil(len(account_list)/batch_size)\n",
    "start_batch = 1\n",
    "\n",
    "print(str(num_of_acc)+' accounts will be processed in '+str(num_of_batches)+' batches of '+str(batch_size))\n",
    "\n",
    "for chunk in batch(account_list,batch_size):\n",
    "    \n",
    "    print(\"Batch \"+str(start_batch)+\" started:\")\n",
    "    \n",
    "    acc_list = \"','\".join(chunk)\n",
    "        \n",
    "    login_sf = sf(username = 'intl_sales_ops_analysts@groupon.com', password = 'BA_intl_grp_3', security_token = 'ryDPBLSTkb54qbEY06GJSiA0')\n",
    "    soql_output = login_sf.bulk.Account.query(f\"SELECT Account_ID_18__c,OwnerId,Acct_Owner_Change_Date__c FROM Account where Account_ID_18__c in ('{acc_list}') limit 200000\")\n",
    "\n",
    "    dic = {}\n",
    "    dic['sf_id'] = [ x['Account_ID_18__c'] for x in soql_output ]\n",
    "    dic['sf_OwnerId'] = [ x['OwnerId'] for x in soql_output ]\n",
    "    dic['sf_Acct_Owner_Change_Date__c'] = [ x['Acct_Owner_Change_Date__c'] for x in soql_output ]\n",
    "        \n",
    "    current_owner_table = pd.DataFrame(dic, columns=dic.keys())\n",
    "\n",
    "    ### process changes:\n",
    "    if not bumblebee.empty:\n",
    "        ### The filtered table is ready for upload to SF\n",
    "        # This joins the current_owner_table and the bumblebee table\n",
    "        df = bumblebee.merge(current_owner_table, left_on='account_id', right_on='sf_id', how='left')\n",
    "\n",
    "        # this filters the table on current_owner_id in Teradata compared to owner_id in SF, and also on the intended employee_sf_id to the current owner_id in SF\n",
    "        filtered_table = df.drop(df[(df['current_owner_id'] != df['sf_OwnerId']) | (df['sf_OwnerId'] == df['employee_sf_id']) | (df['sf_Acct_Owner_Change_Date__c'] == today)].index)\n",
    "\n",
    "        # this drops unnecesary columns\n",
    "        filtered_table.drop('current_owner_id', axis=1, inplace=True)\n",
    "        filtered_table.drop('sf_OwnerId', axis=1, inplace=True)\n",
    "        filtered_table.drop('sf_Acct_Owner_Change_Date__c', axis=1, inplace=True)\n",
    "        filtered_table.drop('sf_id', axis=1, inplace=True)\n",
    "        \n",
    "        # columns are renamed to fit Salesforce names\n",
    "        filtered_table.columns = ['Id','OwnerId']\n",
    "\n",
    "        \n",
    "        print(str(len(filtered_table))+' accounts to be processed in this batch')\n",
    "        \n",
    "        ready_for_upload = []\n",
    "\n",
    "        for row in filtered_table.itertuples():\n",
    "            d = row._asdict()\n",
    "            del d['Index']\n",
    "            ready_for_upload.append(d)\n",
    "            \n",
    "        login_sf.bulk.Account.update(ready_for_upload,batch_size=1000,use_serial=False)\n",
    "        \n",
    "        print(\"Batch \"+str(start_batch)+\" of \"+str(num_of_batches)+\" completed!\")\n",
    "        \n",
    "        start_batch = start_batch+1\n",
    "    \n",
    "        \n",
    "print(\"All batches completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414c148522c3452b8bca3bc99b333facd4b5aaa0d0e2364d6105bb7336ba2f62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
